{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“.conda (Python 3.11.14)”的单元格需要ipykernel包。\n",
      "\u001b[1;31m将“ipykernel”安装到 Python 环境中。\n",
      "\u001b[1;31m命令:“conda install -n .conda ipykernel --update-deps --force-reinstall”"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a575c164214454cdd2f8b8f9048f4cbbc1d8ba2"
   },
   "source": [
    "First things first. Let's take a look at the columns to check for missing or weird values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "telecom_data = pd.read_csv(\"../input/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "telecom_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "279082e82ce4f673a91f8fbe17770ad471dd03a3"
   },
   "source": [
    "No missing values , a very very clean dataset. Not the most real world conditions but it makes our job easier. \n",
    "\n",
    "Let's take a look at the first 5 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ffa6c7dc7ec1986215c387893d7ecf2e46bcafa7",
    "scrolled": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "telecom_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf4de3f0680f624013c34737424714c1936b8840"
   },
   "source": [
    "I see that unlike all the other binary categorical features , senior citizen has alreayd been encoded. I'm going to reverse that for the visualization part of this notebook as It'll be taken care of during the pre-processing stage with all the other categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0533ec4799f3e85361703988ca4289fb70bc5b2c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "telecom_data = telecom_data.replace( { 'SeniorCitizen': { 0: 'No', 1:'Yes' } } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f61966bb7e16dcee08b9e6963be86527c55ab361"
   },
   "source": [
    "I'm gonna write a function that accepts the name of a column (feature) as it's argument and groups the dataframe on that feature giving us the total amount of churned users with respect to each value in that feature and what percent of churned users does each value constitue for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "681854eee7b614150429efd3479a756c3ac10292",
    "scrolled": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def categorical_segment(column_name:str) -> 'grouped_dataframe':\n",
    "    segmented_df = telecom_data[[column_name, 'Churn']]\n",
    "    segmented_churn_df = segmented_df[segmented_df['Churn'] == 'Yes']\n",
    "    grouped_df = segmented_churn_df.groupby(column_name).count().reset_index().rename(columns = {'Churn':'Churned'})\n",
    "    total_count_df = segmented_df.groupby(column_name).count().reset_index().rename(columns = {'Churn':'Total'})\n",
    "    merged_df = pd.merge(grouped_df, total_count_df, how = 'inner', on = column_name)\n",
    "    merged_df['Percent_Churned'] = merged_df[['Churned','Total']].apply(lambda x: (x[0] / x[1]) * 100, axis=1) \n",
    "    return merged_df\n",
    "\n",
    "categorical_columns_list = list(telecom_data.columns)[1:5] + list(telecom_data.columns)[6:18]\n",
    "\n",
    "grouped_df_list = []\n",
    "\n",
    "for column in categorical_columns_list:\n",
    "    grouped_df_list.append( categorical_segment( column ) )\n",
    "    \n",
    "grouped_df_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f9701c1ce8215d30a57d95978cc9de0f8c6e515"
   },
   "source": [
    "**Churn by categorical features**\n",
    "\n",
    "Wrting a loop that goes through each categorical feature and calls the above function to get a grouped dataframe for that feature and visualizes it using a stacked bargraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71a7fad742066d42ed92bd55f0bff30e2bfe330e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "for i , column in enumerate(categorical_columns_list):\n",
    "    fig, ax = plt.subplots(figsize=(13,5))\n",
    "    plt.bar(grouped_df_list[i][column] , [ 100 - i for i in grouped_df_list[i]['Percent_Churned'] ],width = 0.1, color = 'g')\n",
    "    plt.bar(grouped_df_list[i][column],grouped_df_list[i]['Percent_Churned'], bottom =  [ 100 - i for i in grouped_df_list[i]['Percent_Churned'] ],\n",
    "            width = 0.1, color = 'r')\n",
    "    plt.title('Percent Churn by ' + column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Percent Churned')\n",
    "    plt.legend( ('Retained', 'Churned') )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d0dd81a88b2f02e775367065c51b6acd1def223"
   },
   "source": [
    "Some interesting observations:\n",
    "* A higher percentage of senior citizens churned over people who were not senior citizens \n",
    "* More people with partners churned over people who did not have partners. Same for people with dependents \n",
    "* The highest churn by internet service is accounted for by Fiber Optic connection users over people who use the good ol DSL and people not using the internet service. Possibly something to do with price, I would assume.\n",
    "\n",
    "Feel free to take a gander at all the other plots to see what you find interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eaab73235de94d2fd9113824dc49e332c4412080"
   },
   "source": [
    "**Churn by numerical features**\n",
    "\n",
    "Wrting a function like above that groups by numerical features and using a similar for loop to plot one of my favorite plots namely violin plots to visualie churn by  monthly charges and tenure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c098ebbb5020db620eb3b2895cfa1b882fc57ef",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def continous_var_segment(column_name:str) -> 'segmented_df':\n",
    "    segmented_df = telecom_data[[column_name, 'Churn']]\n",
    "    segmented_df = segmented_df.replace( {'Churn': {'No':'Retained','Yes':'Churned'} } )\n",
    "    segmented_df['Customer'] = ''\n",
    "    return segmented_df\n",
    "\n",
    "continous_columns_list = [list(telecom_data.columns)[18]] + [list(telecom_data.columns)[5]]\n",
    "\n",
    "\n",
    "continous_segment_list = []\n",
    "\n",
    "for var in continous_columns_list:\n",
    "    continous_segment_list.append( continous_var_segment(var) )\n",
    "    \n",
    "import seaborn as sns\n",
    "sns.set('talk')\n",
    "\n",
    "for i, column in enumerate( continous_columns_list ):\n",
    "    fig, ax = plt.subplots(figsize=(8,11))\n",
    "    sns.violinplot(x = 'Customer', y = column, data = continous_segment_list[i], hue = 'Churn', split = True)\n",
    "    plt.title('Churn by ' + column)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e93b323488929bad48eaa66b4faecc18a3d88056"
   },
   "source": [
    "Some Interesting observations:\n",
    "* Not suprisingly most retained customers have monthly charges concentrated around 20 Dollars  while most churned customers had monthly charges between 80 - 100 Dollars.\n",
    "* Most of that customers that churned, churned within approximately the first 5-7 months of joining the service while a lot of the retained customers have been around for upwards of 60-65 months ( 5 years and up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "92d040fe09ddc71c6df22b0a3303af3b9c1bf9f7"
   },
   "source": [
    "Normalizing  tenure and monthly charges and using K-means clustering to cluster churned customers based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8265b03d076865363ce6d249924b14bd08fd3895",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "monthlyp_and_tenure = telecom_data[['MonthlyCharges','tenure']][telecom_data.Churn == 'Yes']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "monthly_and_tenure_standardized = pd.DataFrame( scaler.fit_transform(monthlyp_and_tenure) )\n",
    "monthly_and_tenure_standardized.columns = ['MonthlyCharges','tenure']\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 42).fit(monthly_and_tenure_standardized)\n",
    "\n",
    "monthly_and_tenure_standardized['cluster'] = kmeans.labels_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,8))\n",
    "plt.scatter( monthly_and_tenure_standardized['MonthlyCharges'], monthly_and_tenure_standardized['tenure'],\n",
    "           c = monthly_and_tenure_standardized['cluster'], cmap = 'Spectral')\n",
    "\n",
    "plt.title('Clustering churned users by monthly Charges and tenure')\n",
    "plt.xlabel('Monthly Charges')\n",
    "plt.ylabel('Tenure')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "058844a63c1a6adbfeea94ff6dfa3d0a80f80dad"
   },
   "source": [
    "Seems our algorithm found one tight and 2 semi-loose clusters to group churned users by:\n",
    "* *Customers with low monthly charges and low tenure*:  Could have been a temporary connection  for them or people looking for very minimal service who found a service provider offering even lower charges for basic services and churned quickly despite low monthly charges.\n",
    "* *Customers with high monthly charges and low tenure:* The heaviest concentration of churned users. The most common churned users who were possibly unhappy with the prices and stayed for a little while before quickly leaving the service provider for better , cheaper options.\n",
    "* *Customers with high monthly charges and high tenure:* The most interesting group of churned users. They might have stayed initially despite high prices becuase they either thought the service was worth the price or simply due to lack of better alternatives and churned after a while in contrast with most other churned users who churned pretty quickly in their tenure. \n",
    "\n",
    "The interesting observation here is that most churned users with low monthly charges churned pretty quickly. There is a very small concentration of churned users who had low prices in the high tenure zone. This usually points to very dissatisfied customers or customers who were looking for temporary service providers at the time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f34eac19458844bb5795d4c8499b33787237e41"
   },
   "source": [
    "Pre-processing the data using label encoding and one hot encoding to get it ready for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8cb6a81305e3b70f910bdd3649c90773215ede0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "telecom_data_filtered = telecom_data.drop(['TotalCharges','customerID'], axis = 1)\n",
    "\n",
    "def encode_binary(column_name:str):\n",
    "    global telecom_data_filtered\n",
    "    telecom_data_filtered = telecom_data_filtered.replace( { column_name: { 'Yes': 1 , 'No': 0 } }  )\n",
    "    \n",
    "\n",
    "binary_feature_list = list(telecom_data_filtered.columns)[1:4] + [list(telecom_data_filtered.columns)[5]] \\\n",
    "+ [list(telecom_data_filtered.columns)[15]]  \\\n",
    "+ [list(telecom_data_filtered.columns)[18]]\n",
    "    \n",
    "for binary_feature in binary_feature_list:\n",
    "    encode_binary(binary_feature)\n",
    "    \n",
    "\n",
    "telecom_data_processed = pd.get_dummies( telecom_data_filtered, drop_first = True )\n",
    "\n",
    "telecom_data_processed.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fac361ecb78eca7a10bd94ebe22c0f78904e5fbb"
   },
   "source": [
    "Not an very imbalanced dataset in terms of the positive and negative target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29adcb7f7d70e7fedfce8307f03a378b9e52cec7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "telecom_data.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ea8614ab7c10465e03897ff048e44acbfc7f030"
   },
   "source": [
    "Importing necessary libraries and writing a function that makes it easily to assess and visualize model perfomance using sklearn metrics and matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c93f40b17496e3db03c68d3bea0133bb74127686",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as metrics\n",
    "%matplotlib inline \n",
    "\n",
    "X = np.array( telecom_data_processed.drop( ['Churn'] , axis = 1 ) )\n",
    "y = np.array( telecom_data_processed['Churn'] )\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split( X, y , test_size = 0.2 , random_state = 42 )\n",
    "\n",
    "def get_metrics( model ):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    y_actual = y_test \n",
    "    print()\n",
    "    print('-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n",
    "    print()\n",
    "    print('Accuracy on unseen hold out set:' , metrics.accuracy_score(y_actual,y_pred) * 100 , '%' )\n",
    "    print()\n",
    "    f1_score = metrics.f1_score(y_actual,y_pred)\n",
    "    precision = metrics.precision_score(y_actual,y_pred)\n",
    "    recall = metrics.recall_score(y_actual,y_pred)\n",
    "    score_dict = { 'f1_score':[f1_score], 'precision':[precision], 'recall':[recall]}\n",
    "    score_frame = pd.DataFrame(score_dict)\n",
    "    print(score_frame)\n",
    "    print()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( y_actual, y_prob[:,1] ) \n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    plt.plot( fpr, tpr, 'b-', alpha = 0.5, label = '(AUC = %.2f)' % metrics.auc(fpr,tpr) )\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend( loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4b9887f7711cfc6068da428f7d679b961267f5a"
   },
   "source": [
    "Model 1: Random Forest \n",
    "* Tuning min_samples_split, min_samples_leaf, max_features and max_depth using Grid Search with 5 fold cross validation to get a good bias vs variance tradeoff from our model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ec1854127bffd48132c6121cd8d8d81d190ab2d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier( n_estimators = 20, n_jobs=-1, max_features = 'sqrt', random_state = 42 )\n",
    "param_grid1 = {\"min_samples_split\": np.arange(2,11), \n",
    "              \"min_samples_leaf\": np.arange(1,11)}\n",
    "rf_cv = GridSearchCV( rf, param_grid1, cv=5, iid = False )\n",
    "rf_cv.fit(X_train,y_train)\n",
    "print( rf_cv.best_params_ )\n",
    "print( rf_cv.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3da1f5de828f024ddc080aa624160a8cbe77301f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier( n_estimators = 20, n_jobs = -1, min_samples_split = 2, min_samples_leaf = 8, random_state = 2 )\n",
    "param_grid2 = {'max_depth': np.arange(9,21),\n",
    "              'max_features':['sqrt','log2']}\n",
    "rf_cv = GridSearchCV(rf, param_grid2, cv=5, iid = False)\n",
    "rf_cv.fit(X_train,y_train)\n",
    "print( rf_cv.best_params_ )\n",
    "print( rf_cv.best_score_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b850e5e1a8eb4512a407d85e4e081627b9ea1d5"
   },
   "source": [
    "**Note:** \n",
    "* If you think about it from the perspective of a big telecom company , it would be more important to catch most of the postive cases, that is the users who might churn and provide incentives to them to make them stay than it is to catch the customers who will not be churning for now. This means there is a higher cost associated with misclassifying a churned user as retained than there is for missclassifying a retained user as churned. As a result for all models from here on I will focus on improving the recall to maximize the amount of positive cases our model catches. For this I would specify a higher class weight for the positive class '1' and lower very slightly the weight associated with the negative class '0'. This would most likely lower the overall accuracy of the model but as any good data scientist knows, accuracy is not everything. \n",
    "* I will plot the ROC curve and specify the AUC for each model to see how well they were able to separate the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a83cb6093d957b3887d42a19e56f5a7ba3816b2f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier( n_estimators = 1000, max_features = 'log2', max_depth = 11, min_samples_split = 2, \n",
    "                          min_samples_leaf = 8, n_jobs = -1 , random_state = 42, class_weight = {0:0.95, 1:2})\n",
    "rf.fit(X_train,y_train)\n",
    "print('Training Accuracy:',rf.score(X_train,y_train)*100,'%')\n",
    "get_metrics(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00b3f963850e0ab7a43247feae487864c557879c"
   },
   "source": [
    "In the unseen/validation set our random forest model was able to capture 75.6% of all positive cases and has an AUC of 0.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd04d5c666615ed9fa7a5c7512df0c090a74fc6f"
   },
   "source": [
    "Model 2: Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97f6af19df5782c75fc0cc86b12b0b728f8fa8ad",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline( steps = [( 'normalizer', MinMaxScaler() ), \n",
    "                                   ( 'log_reg', LogisticRegression( penalty = 'l2', random_state = 42 ) ) ] )\n",
    "param_dict = dict( log_reg__C = [0.001, 0.01, 0.1, 1, 10, 100])\n",
    "estimator = GridSearchCV( model_pipeline, param_grid = param_dict, cv = 5, n_jobs = -1, iid = False )\n",
    "estimator.fit(X_train,y_train)\n",
    "print(estimator.best_params_)\n",
    "print(estimator.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18762d643d2d7d0cbcca902be202833c006a583c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline( steps = [( 'normalizer', MinMaxScaler() ), \n",
    "                                   ( 'log_reg', LogisticRegression( penalty = 'l2', C = 100, random_state = 42, class_weight = {0:.95 , 1:2} ) ) ] )\n",
    "model_pipeline.fit(X_train,y_train)\n",
    "print('Training Accuracy:',model_pipeline.score(X_train,y_train)*100,'%')\n",
    "get_metrics(model_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f501f5b579e0ad7f4e272b2b1b6f957fb83bbb35"
   },
   "source": [
    "Our logistic regression preformed slighly better than our ensemble method by catching 76.6% of all postive cases in our unseen/validation set but had a very slighly lower AUC of 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "61014280cad3daa73bfe80cb847c3aec86e237d9"
   },
   "source": [
    "Model 3: SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c09236d72d18e01b606e212d12294b8ad9b7400d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "svc_pipeline = Pipeline( steps = [( 'normalizer', MinMaxScaler() ), \n",
    "                                   ( 'svc', SVC(random_state = 42, probability = True) ) ] )\n",
    "params = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_dict = dict( svc__C = params, svc__gamma = params)\n",
    "estimator = GridSearchCV( svc_pipeline, param_grid = param_dict, cv = 5, n_jobs = -1, iid = False )\n",
    "estimator.fit(X_train,y_train)\n",
    "print(estimator.best_params_)\n",
    "print(estimator.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c764dfbd4b8521708b10413b2dcbab682194f12d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(C = 1, gamma = 0.01, class_weight = {0:1, 1:2}, random_state = 42, probability = True)\n",
    "svc.fit(X_train,y_train) \n",
    "print('Training Accuracy:',svc.score(X_train,y_train)*100,'%')\n",
    "get_metrics(svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "657ec33f37da71986acaae2bfcf11cad2642bde9"
   },
   "source": [
    "Our SVC performed worse than the logistic regression and random forest model by catching 71.8% of the positive cases in the unseen/validation set and had an AUC of 0.84. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be4bfaca7b6bd847e8cbe27a60c9507a5a9154eb"
   },
   "source": [
    "Since our logistic regression and random forest model perfomed more or less the same, It's a good idea to go with the simpler and more intepretable model that is the logistic regression for the choice of model. \n",
    "\n",
    "Now let's take a look under the hood of our logistic regression model to see what it has learned by plotting the various coefficients it assigned to all the features. Taking a look at what our model has learned is also a very good way to assess a model before you deploy it for any kind of production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10dd8d09c6b9f5e3c6731454ddb45c75b4128222",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "coeff = model_pipeline.named_steps['log_reg'].coef_.flatten()\n",
    "coeff_updated = np.append( coeff[:8], [sum(coeff[8:10]), sum(coeff[10:12]), sum(coeff[12:14]),sum(coeff[14:16]), \n",
    "                         sum(coeff[16:18]), sum(coeff[18:20]), sum(coeff[20:22]), sum(coeff[22:24]), sum(coeff[24:26]), sum(coeff[26:]) ] )\n",
    "columns = ['SeniorCitizen', 'Partner', 'Dependents', 'Tenure', 'PhoneService', 'PaperlessBilling', 'MonthlyCharges', 'Gender', 'MultipleLines',\n",
    "          'InternetService', 'OnlineSecurity','OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV','StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "fig, ax = plt.subplots(figsize=(50,20))\n",
    "plt.plot(columns, coeff_updated, c = 'yellow', marker='o', linewidth = 6, linestyle='dashed', markersize=20, mfc = 'red')\n",
    "plt.title('Coefficients Learned by the Logistic Regression Model')\n",
    "plt.ylabel('Coeff')\n",
    "plt.xlabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca372c7bccf883f674977c971f651dfeac50a132"
   },
   "source": [
    "Our model assigned very high coefficients to Monthly Charges, Tenure and Internet service which was something I was expecting and surprisingly a high coefficient for contract as well, while all the other coeffcients were pushed close to 0 by our L2 regularization parameter. Although I used C=100 which is considered a pretty large value which means less regularization we can clearly see what our logistic regression model learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb513388ce8f773051aa65e23f34ec39d23747a6"
   },
   "source": [
    "Thank you very much if you took a look at my Kernel. I love feedback and I thrive on criticism. If I did something wrong or stupid or if you simply have nay suggestions for stuff I can improve on, no one would be more happy than me to hear about it. \n",
    "Cheers :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c38706467f141efc59d99dd0d895d11f0a41f003",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
